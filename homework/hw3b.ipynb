{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# HW3B - Pandas Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "See Canvas for details on how to complete and submit this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "This assignment transitions you from NumPy's numerical array operations to Pandas' powerful tabular data manipulation. While NumPy excels at homogeneous numerical arrays, Pandas is designed for the heterogeneous, labeled data that characterizes most real-world datasets—mixing dates, categories, numbers, and text within the same table.\n",
    "\n",
    "You'll work with real bike share data from Chicago's Divvy system to answer questions about urban transportation patterns. Through three progressively complex problems—exploring usage patterns, analyzing rider behavior, and conducting temporal analysis—you'll discover why Pandas has become the standard tool for data analysis in Python.\n",
    "\n",
    "The assignment emphasizes Pandas' design philosophy: named column access, explicit indexing methods (loc/iloc), handling missing data, and method chaining for readable data pipelines. You'll also see how Pandas builds on NumPy while adding the structure and convenience needed for practical data science work.\n",
    "\n",
    "This assignment should take 3-5 hours to complete.\n",
    "\n",
    "Before submitting, ensure your notebook:\n",
    "\n",
    "- Runs completely with \"Kernel → Restart & Run All\"\n",
    "- Includes thoughtful responses to all interpretation questions\n",
    "- Uses clear variable names and follows good coding practices\n",
    "- Shows your work (don't just print final answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "By completing this assignment, you will be able to:\n",
    "\n",
    "1. **Construct and manipulate Pandas data structures**\n",
    "   - Create DataFrames from dictionaries and CSV files\n",
    "   - Distinguish between Series and DataFrame objects\n",
    "   - Set and reset index structures appropriately\n",
    "   - Understand when operations return views vs copies\n",
    "2. **Apply explicit indexing paradigms**\n",
    "   - Use `loc[]` for label-based data access\n",
    "   - Use `iloc[]` for position-based data access\n",
    "   - Access columns using bracket notation\n",
    "   - Explain when each indexing method is appropriate\n",
    "3. **Diagnose and explore datasets systematically**\n",
    "   - Use `info()`, `describe()`, `head()`, and `dtypes` to understand data structure\n",
    "   - Identify missing values with `isna()` and `notna()`\n",
    "   - Calculate summary statistics across different axes\n",
    "   - Interpret value distributions with `value_counts()`\n",
    "4. **Filter data with boolean indexing and queries**\n",
    "   - Combine multiple conditions with `&`, `|`, and `~` operators\n",
    "   - Use `isin()` for membership testing\n",
    "   - Apply `query()` for readable complex filters\n",
    "   - Understand how index alignment affects operations\n",
    "5. **Work with datetime data**\n",
    "   - Parse dates during CSV loading\n",
    "   - Extract temporal components with the `.dt` accessor\n",
    "   - Filter data by date ranges\n",
    "   - Create time-based derived features\n",
    "6. **Connect Pandas patterns to data analysis workflows**\n",
    "   - Formulate questions that data can answer\n",
    "   - Choose appropriate methods for different analysis tasks\n",
    "   - Interpret results in domain context\n",
    "   - Recognize when vectorized operations outperform apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Generative AI Allowance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "You may use GenAI tools for brainstorming, explanations, and code sketches if you disclose it, understand it, and validate it. Your submission must represent your own work and you are solely responsible for its correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Total of 90 points available, will be graded out of 80. Scores of >100% are allowed.\n",
    "\n",
    "Distribution:\n",
    "\n",
    "- Tasks: 48 pts\n",
    "- Interpretation: 32 pts\n",
    "- Reflection: 10 pts\n",
    "\n",
    "Points by Problem:\n",
    "\n",
    "- Problem 1: 3 tasks, 10 pts\n",
    "- Problem 2: 4 tasks, 14 pts\n",
    "- Problem 3: 4 tasks, 14 pts\n",
    "- Problem 4: 3 tasks, 10 pts\n",
    "\n",
    "Interpretation Questions:\n",
    "\n",
    "- Problem 1: 3 questions, 8 pts\n",
    "- Problem 2: 4 questions, 8 pts\n",
    "- Problem 3: 3 questions, 8 pts\n",
    "- Problem 4: 3 questions, 8 pts\n",
    "\n",
    "Graduate differentiation: poor follow-up responses will result in up to a 5pt deduction for that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Dataset: Chicago Divvy Bike Share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The dataset you will analyze is based on real trip information from Divvy, Chicago's bike share system. It contains individual trips with start/end times, station information, and rider type.\n",
    "\n",
    "Dataset homepage: https://divvybikes.com/system-data\n",
    "\n",
    "Each trip includes:\n",
    "\n",
    "- Trip start and end times (datetime)\n",
    "- Start and end station names and IDs\n",
    "- Rider type (member vs casual)\n",
    "- Bike type (classic, electric, or docked)\n",
    "\n",
    "Chicago's Department of Transportation uses this data to optimize station placement, understand usage patterns, and improve service. You'll explore similar questions that real transportation analysts investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Problem 1: Creating DataFrames from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Before loading data from files, you need to understand how Pandas structures are built. In this problem, you'll create Series and DataFrames manually using Python's built-in data structures. This is a quick warmup to establish the fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Task 1a: Create a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Create a Series called `temperatures` representing daily high temperatures for a week:\n",
    "\n",
    "- Monday: 72°F\n",
    "- Tuesday: 75°F  \n",
    "- Wednesday: 68°F\n",
    "- Thursday: 71°F\n",
    "- Friday: 73°F\n",
    "\n",
    "Use the day names as the index. Print the Series and its data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday       72\n",
      "Tuesday      75\n",
      "Wednesday    68\n",
      "Thursday     71\n",
      "Friday       73\n",
      "dtype: int64\n",
      "\n",
      " Series data type: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temperatures = pd.Series({'Monday': 72, 'Tuesday': 75, 'Wednesday': 68, 'Thursday': 71, 'Friday': 73})\n",
    "print(temperatures)\n",
    "\n",
    "# A Series data type is printed automatically when the Series is printed. To be more explicit, an F print function \n",
    "# will be used to label the data type. \n",
    "\n",
    "print(f\"\\n Series data type: {temperatures.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Task 1b: Create a DataFrame from a Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb764d-fcf5-4c18-99cf-0a7ce7f5c954",
   "metadata": {},
   "source": [
    "Create a DataFrame called `products` with the following data:\n",
    "\n",
    "| product | price | quantity |\n",
    "|---------|-------|----------|\n",
    "| Widget  | 19.99 | 100 |\n",
    "| Gadget  | 24.99 | 75 |\n",
    "| Doohickey | 12.49 | 150 |\n",
    "\n",
    "Use a dictionary where keys are column names and values are lists. Print the DataFrame and report its shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     product  price  quantity\n",
      "0     Widget  19.99       100\n",
      "1     Gadget  24.99        75\n",
      "2  Doohickey  12.49       150\n",
      "\n",
      " The 'products' DataFrame shape is (3, 3)\n"
     ]
    }
   ],
   "source": [
    "products_dictionary = {\n",
    "    'product': ['Widget', 'Gadget', 'Doohickey'],\n",
    "    'price': [19.99, 24.99, 12.49],\n",
    "    'quantity': [100, 75, 150]\n",
    "}\n",
    "\n",
    "products = pd.DataFrame(products_dictionary)\n",
    "\n",
    "print(products)\n",
    "print(f\"\\n The 'products' DataFrame shape is {products.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Task 1c: Access DataFrame Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Using the `products` DataFrame from Task 1b, extract and print:\n",
    "\n",
    "1. The `price` column as a Series\n",
    "2. The `product` and `quantity` columns as a DataFrame (using a list of column names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'price' column as a Series:\n",
      "\n",
      "0    19.99\n",
      "1    24.99\n",
      "2    12.49\n",
      "Name: price, dtype: float64\n",
      "\n",
      "The 'product' and 'quantity' columns as a DataFrame:\n",
      "\n",
      "     product  quantity\n",
      "0     Widget       100\n",
      "1     Gadget        75\n",
      "2  Doohickey       150\n"
     ]
    }
   ],
   "source": [
    "print(f\"The 'price' column as a Series:\")\n",
    "print(f\"\\n{products['price']}\")\n",
    "print(f\"\\nThe 'product' and 'quantity' columns as a DataFrame:\")\n",
    "print(f\"\\n{products[['product', 'quantity']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Answer the following questions (briefly / concisely) in the markdown cell below:\n",
    "\n",
    "1. Data structure mapping: When you create a DataFrame from a dictionary (like in Task 1b), what do the dictionary keys become? What do the values become?\n",
    "2. Bracket notation: Why does `df['price']` return a Series, but `df[['price']]` return a DataFrame? What's the difference in what you're asking for?\n",
    "3. Index purpose: In Task 1a, you used day names as the index instead of default numbers (0, 1, 2...). When would a custom index like this be more useful than the default numeric index?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "*Problem 1 interpretation here*\n",
    "1. From my notes: \"a DataFrame can be thought of as a dictionary of Series with a shared index. As such, it is common to construct a DataFrame from a Python dictionary, where keys are the column names and values are equal-length lists\". Therefore, the keys of a python dictionary become the DataFrame column names, and the values of a Python dictionary become the rows for the associated key column name.\n",
    "2. df['price'] returns a series because we are accessing the 'price' column via a \"1D NumPy array\". In Pandas, a 1D array is represented as a series. df[['price']] returns a DataFrame becuase we are accessing the 'price' column via a \"2D NumPy array\". In Pandas, a 2D array is represented as a DataFrame. This philosophy mirrors how indexing works in NumPy.\n",
    "3. Custom indexes are more useful than the default numeric indexes when they can provide more context and description, when the numeric index doesn't represent a \"standard\" starting point, or when the presented data \"jumps around\" compared to an established \"standard\". For example, the custom indexes are useful in this case because without context, someone could misinterpret that 0 represents Saturday or Sunday, and not Monday. Additionally, not in this problem, but if data was presented in a mixed order compared to normal (e.g., Wednesday, Monday, Thursday, Friday, Tuesday), the custom indexes would be more useful than default numeric indexes that have no meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Problem 2: Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Before starting this problem, make sure you are working in a copy of this file in the `my_repo` folder you created in HW2a. You must also have a copy of the file `202410-divvy-tripdata-100k.csv` in a subdirectory called `data`. That file structure is illustrated below.\n",
    "\n",
    "```text\n",
    "~/insy6500/my_repo\n",
    "└── homework\n",
    "    ├── data\n",
    "    │   └── 202410-divvy-tripdata-100k.csv\n",
    "    └── hw3b.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Task 2a: Load and Understand Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Start by loading the data \"as-is\" to get a general understanding of the overall structure and how Pandas interprets it by default.\n",
    "\n",
    "Note on file paths: The provided code uses `Path` from Python's `pathlib` module to handle file paths. Path objects work consistently across operating systems (Windows uses backslashes `\\`, Mac/Linux use forward slashes `/`), automatically using the correct separator for your system. The provided code defines `csv_path` which should be used as the filename in your `pd.read_csv` to load the data file.\n",
    "\n",
    "1. Use `pd.read_csv` to load `csv_path` (provided below) without specifying any other arguments. Assign it to the variable `df_raw`.\n",
    "2. Use the methods we described in class to explore the shape, structure, types, etc. of the data. In particular, consider which columns represent dates or categories.\n",
    "3. Note the amount of memory used by the dataset. See the section on memory diagnostics in notebook 07a for appropriate code snippets using `memory_usage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions: 2\n",
      "\n",
      "\n",
      "Data Types:\n",
      "\n",
      "ride_id                object\n",
      "rideable_type          object\n",
      "started_at             object\n",
      "ended_at               object\n",
      "start_station_name     object\n",
      "start_station_id       object\n",
      "end_station_name       object\n",
      "end_station_id         object\n",
      "start_lat             float64\n",
      "start_lng             float64\n",
      "end_lat               float64\n",
      "end_lng               float64\n",
      "member_casual          object\n",
      "dtype: object\n",
      "\n",
      "Data Frame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   ride_id             100000 non-null  object \n",
      " 1   rideable_type       100000 non-null  object \n",
      " 2   started_at          100000 non-null  object \n",
      " 3   ended_at            100000 non-null  object \n",
      " 4   start_station_name  89623 non-null   object \n",
      " 5   start_station_id    89623 non-null   object \n",
      " 6   end_station_name    89485 non-null   object \n",
      " 7   end_station_id      89485 non-null   object \n",
      " 8   start_lat           100000 non-null  float64\n",
      " 9   start_lng           100000 non-null  float64\n",
      " 10  end_lat             99913 non-null   float64\n",
      " 11  end_lng             99913 non-null   float64\n",
      " 12  member_casual       100000 non-null  object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 9.9+ MB\n",
      "\n",
      "Here we see the data consists of 13 columns, four of which are the integer type and the remainder object type. There are 100,000 rows, but not every column has a valid entry.\n",
      "\n",
      "Let's look at the first five data entries to better understand the data types:\n",
      "\n",
      "            ride_id  rideable_type               started_at  \\\n",
      "0  67BB74BD7667BAB7  electric_bike  2024-09-30 23:12:01.622   \n",
      "1  5AF1AC3BA86ED58C  electric_bike  2024-09-30 23:19:25.409   \n",
      "2  7961DD2FC1280CDC   classic_bike  2024-09-30 23:32:24.672   \n",
      "3  2E16892DEEF4CC19   classic_bike  2024-09-30 23:42:11.207   \n",
      "4  AAF0220F819BEE01  electric_bike  2024-09-30 23:49:25.380   \n",
      "\n",
      "                  ended_at         start_station_name start_station_id  \\\n",
      "0  2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave           bdd4c3   \n",
      "1  2024-10-01 00:42:09.933                        NaN              NaN   \n",
      "2  2024-10-01 00:23:18.647     St. Clair St & Erie St           9c619a   \n",
      "3  2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave           72a04d   \n",
      "4  2024-10-01 00:06:27.476          900 W Harrison St           11da85   \n",
      "\n",
      "           end_station_name end_station_id  start_lat  start_lng    end_lat  \\\n",
      "0                       NaN            NaN  42.012342 -87.688243  41.970000   \n",
      "1    Benson Ave & Church St         a10cf0  42.070000 -87.730000  42.048214   \n",
      "2  LaSalle St & Illinois St         fbd1ad  41.894345 -87.622798  41.890762   \n",
      "3    Loomis St & Archer Ave         896337  41.895954 -87.667728  41.841633   \n",
      "4         900 W Harrison St         11da85  41.874754 -87.649807  41.874754   \n",
      "\n",
      "     end_lng member_casual  \n",
      "0 -87.650000        casual  \n",
      "1 -87.683485        casual  \n",
      "2 -87.631697        member  \n",
      "3 -87.657435        casual  \n",
      "4 -87.649807        member  \n",
      "\n",
      "It looks like ride_id is a unique identifier, rideable_type describes what kind of bike was ridden, started_at and ended_at are date time objets, start_station_id appears to be a unique identifier for start_station_name, similar for end_station_id and end_station_name, start/stop_lat/lng provides precise GPS data for where trips begin and end, and member_casual describes the membership type of the user. We can further explor some columns to see how many unique values there are, telling us how many ride_types, start and end stations, and membership types there are:\n",
      "\n",
      "Unique Ride Types:\n",
      "['electric_bike' 'classic_bike']\n",
      "\n",
      "Number of Unique Start Stations:\n",
      "825\n",
      "\n",
      "Let's confirm there is a unique Start Station ID per Start Station Name:\n",
      "825\n",
      "\n",
      "Similar for End Stations:\n",
      "Number of Unique End Stations: 827\n",
      "Number of Unique End Station IDs: 827\n",
      "\n",
      "Let's see how many membership types there are:\n",
      "['casual' 'member']\n",
      "\n",
      "Lastly, let's explore the memory usage of the data set:\n",
      "Before any data manipulation, the memory usage of the data set is 66799245 bytes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# create a OS-independent pointer to the csv file created by Setup\n",
    "csv_path = Path('./data/202410-divvy-tripdata-100k.csv')\n",
    "\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "# Let's use some basic methods to understand the shape and data type of the csv file:\n",
    "print(f\"Number of dimensions: {df_raw.ndim}\\n\")\n",
    "print(f\"\\nData Types:\\n\")\n",
    "print(f\"{df_raw.dtypes}\\n\")\n",
    "# Dive into initial data expoloration with .info() method to understand the basic information about the DataFrame\n",
    "print(\"Data Frame Info:\")\n",
    "df_raw.info()\n",
    "print(f\"\\nHere we see the data consists of 13 columns, four of which are the integer type and the remainder object type. There are 100,000 rows, but not every column has a valid entry.\")\n",
    "# The describe method does not seem to be extremely useful here, as it provides summary statistics for numeric values. I don't see an immediate need to understand the summary statistics for start and stop longitude/latitude data.\n",
    "# Maybe seeing the first few data entries will help us better understand what we are working with:\n",
    "print(f\"\\nLet's look at the first five data entries to better understand the data types:\\n\")\n",
    "print(df_raw.head())\n",
    "print(f\"\\nIt looks like ride_id is a unique identifier, rideable_type describes what kind of bike was ridden, started_at and ended_at are date time objets, start_station_id appears to be a unique identifier for start_station_name, similar for end_station_id and end_station_name, start/stop_lat/lng provides precise GPS data for where trips begin and end, and member_casual describes the membership type of the user. We can further explor some columns to see how many unique values there are, telling us how many ride_types, start and end stations, and membership types there are:\")\n",
    "print(\"\\nUnique Ride Types:\")\n",
    "print(df_raw['rideable_type'].unique())\n",
    "print(\"\\nNumber of Unique Start Stations:\")\n",
    "print(df_raw['start_station_name'].nunique())\n",
    "print(f\"\\nLet's confirm there is a unique Start Station ID per Start Station Name:\")\n",
    "print(df_raw['start_station_id'].nunique())\n",
    "print(f\"\\nSimilar for End Stations:\")\n",
    "print(f\"Number of Unique End Stations: {df_raw['end_station_name'].nunique()}\")\n",
    "print(f\"Number of Unique End Station IDs: {df_raw['end_station_id'].nunique()}\")\n",
    "print(f\"\\nLet's see how many membership types there are:\")\n",
    "print(df_raw['member_casual'].unique())\n",
    "print(f\"\\nLastly, let's explore the memory usage of the data set:\")\n",
    "df_raw.memory_usage(deep=True).sum()\n",
    "print(f\"Before any data manipulation, the memory usage of the data set is {df_raw.memory_usage(deep=True).sum()} bytes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Task 2b: Reload with Proper Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "1. Repeat step 2a.1 to reload the data. Use the `dtype` and `parse_dates` arguments to properly assign categorical and date types. Assign the result to the variable name `rides`.\n",
    "2. After loading, use `rides.info()` to confirm the type changes.\n",
    "3. Use `memory_usage` to compare the resulting size with that from step 2a.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewing dtype attribute output above, we can turn the rideable_type and member_casual columns into category data types and the started_at and ended_at columns into date object types.\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   ride_id             100000 non-null  object        \n",
      " 1   rideable_type       100000 non-null  category      \n",
      " 2   started_at          100000 non-null  datetime64[ns]\n",
      " 3   ended_at            100000 non-null  datetime64[ns]\n",
      " 4   start_station_name  89623 non-null   object        \n",
      " 5   start_station_id    89623 non-null   object        \n",
      " 6   end_station_name    89485 non-null   object        \n",
      " 7   end_station_id      89485 non-null   object        \n",
      " 8   start_lat           100000 non-null  float64       \n",
      " 9   start_lng           100000 non-null  float64       \n",
      " 10  end_lat             99913 non-null   float64       \n",
      " 11  end_lng             99913 non-null   float64       \n",
      " 12  member_casual       100000 non-null  category      \n",
      "dtypes: category(2), datetime64[ns](2), float64(4), object(5)\n",
      "memory usage: 8.6+ MB\n",
      "\n",
      "Prior to data type manuipulation, the df_raw data set required 66799245 bytes. After uploading the csv file with proper data types, the rides data set requires 39349125 bytes.\n"
     ]
    }
   ],
   "source": [
    "# create a OS-independent pointer to the csv file created by Setup\n",
    "csv_path = Path('./data/202410-divvy-tripdata-100k.csv')\n",
    "print(f\"Reviewing dtype attribute output above, we can turn the rideable_type and member_casual columns into category data types and the started_at and ended_at columns into date object types.\\n\")\n",
    "rides = pd.read_csv(csv_path, dtype={'rideable_type':'category', 'member_casual':'category'}, parse_dates=['started_at', 'ended_at'])\n",
    "rides.info()\n",
    "print(f\"\\nPrior to data type manuipulation, the df_raw data set required {df_raw.memory_usage(deep=True).sum()} bytes. After uploading the csv file with proper data types, the rides data set requires {rides.memory_usage(deep=True).sum()} bytes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Task 2c: Explore Structure and Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Using the `rides` DataFrame from Task 2b:\n",
    "\n",
    "1. Determine the range of starting dates in the dataframe using the `min` and `max` methods.\n",
    "2. Count the number of missing values in each column. See the section of the same name in lecture 06b.\n",
    "3. Convert the Series from step 2 to a DataFrame using `.to_frame(name='count')`, then add a column called 'percentage' that calculates the percentage of missing values for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rides data set contains start date data starting on 2024-09-30 23:12:01.622000 until 2024-10-31 23:54:02.851000.\n",
      "\n",
      "The rides data set contains start date data starting on September 30, 2024 until October 31, 2024.\n",
      "\n",
      "Based on the .info attribute output above, columns, 4, 5, 6, 7, 10 and 11 have missing values.\n",
      "There are 10377 empty columns in column 4: 'start_station_name'\n",
      "There are 10377 empty columns in column 5: 'start_station_id'\n",
      "There are 10515 empty columns in column 6: 'end_station_name'\n",
      "There are 10515 empty columns in column 7: 'end_station_id'\n",
      "There are 87 empty columns in column 10: 'end_lat'\n",
      "There are 87 empty columns in column 11: 'end_lng'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ride_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rideable_type</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started_at</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ended_at</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_station_name</th>\n",
       "      <td>10377</td>\n",
       "      <td>0.10377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_station_id</th>\n",
       "      <td>10377</td>\n",
       "      <td>0.10377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_station_name</th>\n",
       "      <td>10515</td>\n",
       "      <td>0.10515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_station_id</th>\n",
       "      <td>10515</td>\n",
       "      <td>0.10515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_lat</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_lng</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_lat</th>\n",
       "      <td>87</td>\n",
       "      <td>0.00087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_lng</th>\n",
       "      <td>87</td>\n",
       "      <td>0.00087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>member_casual</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count  percentage\n",
       "ride_id                 0     0.00000\n",
       "rideable_type           0     0.00000\n",
       "started_at              0     0.00000\n",
       "ended_at                0     0.00000\n",
       "start_station_name  10377     0.10377\n",
       "start_station_id    10377     0.10377\n",
       "end_station_name    10515     0.10515\n",
       "end_station_id      10515     0.10515\n",
       "start_lat               0     0.00000\n",
       "start_lng               0     0.00000\n",
       "end_lat                87     0.00087\n",
       "end_lng                87     0.00087\n",
       "member_casual           0     0.00000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I referenced my lecture notes to remind myself of the dt accessor, but confirmed with chatGPT that .dt.dates would give me just the date from a datetime64[ns] object.\n",
    "start_dates_series = rides['started_at']\n",
    "min_start_date = start_dates_series.min()\n",
    "max_start_date = start_dates_series.max()\n",
    "print(f\"The rides data set contains start date data starting on {min_start_date} until {max_start_date}.\")\n",
    "# I was interested in how I could present this in a more readable format, so chatGPT provided me with the .strftime() method:\n",
    "print(f\"\\nThe rides data set contains start date data starting on {min_start_date.strftime('%B %d, %Y')} until {max_start_date.strftime('%B %d, %Y')}.\")\n",
    "print(f\"\\nBased on the .info attribute output above, columns, 4, 5, 6, 7, 10 and 11 have missing values.\")\n",
    "print(f\"There are {rides['start_station_name'].isna().sum()} empty columns in column 4: 'start_station_name'\")\n",
    "print(f\"There are {rides['start_station_id'].isna().sum()} empty columns in column 5: 'start_station_id'\")\n",
    "print(f\"There are {rides['end_station_name'].isna().sum()} empty columns in column 6: 'end_station_name'\")\n",
    "print(f\"There are {rides['end_station_id'].isna().sum()} empty columns in column 7: 'end_station_id'\")\n",
    "print(f\"There are {rides['end_lat'].isna().sum()} empty columns in column 10: 'end_lat'\")\n",
    "print(f\"There are {rides['end_lng'].isna().sum()} empty columns in column 11: 'end_lng'\")\n",
    "empty_value_series = rides.isna().sum()\n",
    "empty_value_df = empty_value_series.to_frame(name='count')\n",
    "empty_value_df['percentage'] = empty_value_df['count']/len(rides)\n",
    "empty_value_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Task 2d: Create Trip Duration Column and Set Index\n",
    "\n",
    "Before setting the index, create a derived column for trip duration:\n",
    "\n",
    "1. Calculate trip_duration_min by subtracting `started_at` from `ended_at`, then converting to minutes using `.dt.total_seconds() / 60`\n",
    "3. Display basic statistics (min, max, mean) for the new column using `.describe()`\n",
    "4. Show the first few rows with `started_at`, `ended_at`, and `trip_duration_min` to verify the calculation\n",
    "5. Set `started_at` as the DataFrame's index. Verify the change by printing the index and displaying the first few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The first few rows of derived column 'trip_duration_min':\n",
      "0    67.984200\n",
      "1    82.742067\n",
      "2    50.899583\n",
      "3    28.093733\n",
      "4    17.034933\n",
      "Name: trip_duration_min, dtype: float64\n",
      "\n",
      "2. Basic statistics of 'trip_duration_min' (in minutes):\n",
      "count    100000.000000\n",
      "mean         16.144576\n",
      "std          52.922539\n",
      "min           0.006533\n",
      "25%           5.489271\n",
      "50%           9.423592\n",
      "75%          16.407171\n",
      "max        1499.949717\n",
      "Name: trip_duration_min, dtype: float64\n",
      "\n",
      "3. The first few rows to verify calculations:\n",
      "               started_at                ended_at  trip_duration_min\n",
      "0 2024-09-30 23:12:01.622 2024-10-01 00:20:00.674          67.984200\n",
      "1 2024-09-30 23:19:25.409 2024-10-01 00:42:09.933          82.742067\n",
      "2 2024-09-30 23:32:24.672 2024-10-01 00:23:18.647          50.899583\n",
      "3 2024-09-30 23:42:11.207 2024-10-01 00:10:16.831          28.093733\n",
      "4 2024-09-30 23:49:25.380 2024-10-01 00:06:27.476          17.034933\n",
      "\n",
      "4a. We can confirm the new index is set with 'rides.index':\n",
      "DatetimeIndex(['2024-09-30 23:12:01.622000', '2024-09-30 23:19:25.409000',\n",
      "               '2024-09-30 23:32:24.672000', '2024-09-30 23:42:11.207000',\n",
      "               '2024-09-30 23:49:25.380000', '2024-09-30 23:49:40.016000',\n",
      "               '2024-10-01 00:00:53.414000', '2024-10-01 00:05:44.954000',\n",
      "               '2024-10-01 00:06:12.035000', '2024-10-01 00:10:03.646000',\n",
      "               ...\n",
      "               '2024-10-31 23:36:04.200000', '2024-10-31 23:36:34.956000',\n",
      "               '2024-10-31 23:36:49.500000', '2024-10-31 23:38:20.262000',\n",
      "               '2024-10-31 23:44:03.832000', '2024-10-31 23:44:23.211000',\n",
      "               '2024-10-31 23:44:45.948000', '2024-10-31 23:50:31.160000',\n",
      "               '2024-10-31 23:53:02.355000', '2024-10-31 23:54:02.851000'],\n",
      "              dtype='datetime64[ns]', name='started_at', length=100000, freq=None)\n",
      "\n",
      "4b. Dispalying the first few rows of the rides DataFrame with the new index:\n",
      "                                  ride_id  rideable_type  \\\n",
      "started_at                                                 \n",
      "2024-09-30 23:12:01.622  67BB74BD7667BAB7  electric_bike   \n",
      "2024-09-30 23:19:25.409  5AF1AC3BA86ED58C  electric_bike   \n",
      "2024-09-30 23:32:24.672  7961DD2FC1280CDC   classic_bike   \n",
      "2024-09-30 23:42:11.207  2E16892DEEF4CC19   classic_bike   \n",
      "2024-09-30 23:49:25.380  AAF0220F819BEE01  electric_bike   \n",
      "\n",
      "                                       ended_at         start_station_name  \\\n",
      "started_at                                                                   \n",
      "2024-09-30 23:12:01.622 2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave   \n",
      "2024-09-30 23:19:25.409 2024-10-01 00:42:09.933                        NaN   \n",
      "2024-09-30 23:32:24.672 2024-10-01 00:23:18.647     St. Clair St & Erie St   \n",
      "2024-09-30 23:42:11.207 2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave   \n",
      "2024-09-30 23:49:25.380 2024-10-01 00:06:27.476          900 W Harrison St   \n",
      "\n",
      "                        start_station_id          end_station_name  \\\n",
      "started_at                                                           \n",
      "2024-09-30 23:12:01.622           bdd4c3                       NaN   \n",
      "2024-09-30 23:19:25.409              NaN    Benson Ave & Church St   \n",
      "2024-09-30 23:32:24.672           9c619a  LaSalle St & Illinois St   \n",
      "2024-09-30 23:42:11.207           72a04d    Loomis St & Archer Ave   \n",
      "2024-09-30 23:49:25.380           11da85         900 W Harrison St   \n",
      "\n",
      "                        end_station_id  start_lat  start_lng    end_lat  \\\n",
      "started_at                                                                \n",
      "2024-09-30 23:12:01.622            NaN  42.012342 -87.688243  41.970000   \n",
      "2024-09-30 23:19:25.409         a10cf0  42.070000 -87.730000  42.048214   \n",
      "2024-09-30 23:32:24.672         fbd1ad  41.894345 -87.622798  41.890762   \n",
      "2024-09-30 23:42:11.207         896337  41.895954 -87.667728  41.841633   \n",
      "2024-09-30 23:49:25.380         11da85  41.874754 -87.649807  41.874754   \n",
      "\n",
      "                           end_lng member_casual  trip_duration_min  \n",
      "started_at                                                           \n",
      "2024-09-30 23:12:01.622 -87.650000        casual          67.984200  \n",
      "2024-09-30 23:19:25.409 -87.683485        casual          82.742067  \n",
      "2024-09-30 23:32:24.672 -87.631697        member          50.899583  \n",
      "2024-09-30 23:42:11.207 -87.657435        casual          28.093733  \n",
      "2024-09-30 23:49:25.380 -87.649807        member          17.034933  \n"
     ]
    }
   ],
   "source": [
    "rides['trip_duration_min'] = (rides['ended_at'] - rides['started_at']).dt.total_seconds()/60\n",
    "print(f\"1. The first few rows of derived column 'trip_duration_min':\\n{rides['trip_duration_min'].head()}\")\n",
    "print(f\"\\n2. Basic statistics of 'trip_duration_min' (in minutes):\")\n",
    "print(rides['trip_duration_min'].describe())\n",
    "print(f\"\\n3. The first few rows to verify calculations:\")\n",
    "print(rides[['started_at', 'ended_at', 'trip_duration_min']].head())\n",
    "rides.set_index('started_at', inplace=True)\n",
    "print(f\"\\n4a. We can confirm the new index is set with 'rides.index':\\n{rides.index}\")\n",
    "print(f\"\\n4b. Dispalying the first few rows of the rides DataFrame with the new index:\")\n",
    "print(rides.iloc[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Reflect on problem 2 and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. What types did Pandas assign to `started_at` and `member_casual` in Task 2a? Why might these defaults be problematic?\n",
    "2. Look at the values in the station ID fields. Based on what you learned about git commit IDs in HW3a, how do you think the station IDs were derived?\n",
    "3. Explain in your own words what method chaining is, what `df.isna().sum()` does and how it works.\n",
    "4. Assume you found ~10% missing values in station columns but ~0% in coordinates. What might explain this? How might you handle the affected rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "1. Pandas assigned started_at and member_casual as object data types in Task 2a. These defaults might be problematic because as we demonstrated in the exercise, these data types require significantly more memory to store compared to the 'category' data type. Less memory burden allows for faster, more efficient processing as well as the ability to process larger data sets.\n",
    "2. Looking at the values in the station ID field, and based on what we learned about git commit IDs in HW3a, it appears the station IDs are similar to the unique ID for commits, or more technically, the cryptographic hash, which represents the full content and context of the commit, allowing for more precise traceability. Presumably, this station ID represents a cryptographic hash from the first time that station information was commited to a Git repository, and was used there-on-out as a unique identifier for said station. This utilizes the inherent integrity of Git commit's underlying methodology to ensure traceability is maintained.\n",
    "3. Method chaining allows for multiple operations to be performed on one data set without having to create intermediate variables to track the indivual operations. It is a more precise, efficient way to manipulate data to one's intentions, else, it could require multiple lines of code. df.isna().sum() uses boolean logic to determine if the row value in a column is populaed with legitimate data or NaN. If the boolean evaluates to 1, it confirms there is no data for that row entry in a specific column. The .sum() then sums the boolean evaluation of the entire column. Each 1 represented an absent row entry for that column, therefore, the entire sum represnets the total amount of row entries that were empty for the entire column. Method chaining is critical here, because the operation first has to evaluate whether the row entry is populated or not before taking the sum.\n",
    "4. Based on my knowledge of bike sharing operations, what ~10% of missing values in the station columns but ~0% in the coordinate columns most-likely represents a user who did not start or end his/her trip at a dedicated bike station. Despite this, GPS coordinates can still confirm the location at which the trip started and stopped. To handle these affected rows, instead of leaving the started_at or ended_at column empty, we could develop a terminology or category to signify that the trip did not start or end at a dedicated station. For example \"in_transit\" could represent that a bike was picked up and used or dropped off somewhere within the expected use area but not at a dedicated station.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Compare memory usage results in 2a.3 and 2b.3. What caused the change? Why are these numbers different from what is reported at the bottom of `df.info()`? Which should you use if data size is a concern?\n",
    "\n",
    "Working with DataFrames typically requires 5-10x the dataset size in available RAM. On a system with 16GB, assuming about 30% overhead from the operating system and other programs, what range of dataset sizes would be safely manageable? Calculate using both 5x (optimistic) and 10x (conservative) multipliers, then explain which you'd recommend for reliable work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "In question 2a.3, the df_raw DataFrame required 66799245 bytes. In question 2b.3, the rides DataFrame required 39349125 bytes. The change in memory usage was a result of the dtypes of the data that was uploaded from the csv file. Originally, all the data other than the latitude and longitude coordinates were codes as 'object' data types. After chaning the 'rideable_type' and 'member_casual' columns to 'category' data types and the 'started_at' and 'ended_at' columns to datetime data types, the memory usage decreased significantly (by ~40%!). After some quick research, df.info() only represents an approximate consumption value of the data utilized by Rapid Access Memory (RAM). The 'object' data type require significant memory allocation, that is the primary reason for the approximation. However, the approximation is well under the actual memory use, because referencing the values above, 9.9+ MB technically represents greater than 9.9 MegaBytes (9.9 x 10^6 = 9,900,000 bytes), which is WELL under the calculated values of 66/39 MB. If data size is a concern, one should definitely try to use the category and datetime data types when appropriate. \n",
    "\n",
    "Quick math would indicate that ~ 4.5 GB are required to run the OS and other systems, leaving 11.5 GB available for other uses. Therefore, to safely manage a dataset, the memory should require between 1.15 and 2.3 GB of data. For reliable work, I would definitely recommend using the conservative value of 1.15 GB to allow for sufficient extra memory for any other type of data anlysis work that needs to be done in association with the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Problem 3: Filtering and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "With clean data loaded, you can now filter and transform it to answer specific questions. This problem focuses on Pandas' powerful indexing and filtering capabilities, along with creating derived columns that enable deeper analysis.\n",
    "\n",
    "You'll continue working with the `rides` DataFrame from Problem 2, which has `started_at` set as the index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Task 3a: Boolean Indexing and Membership Testing\n",
    "\n",
    "Use boolean indexing and the `isin()` method to answer these questions:\n",
    "\n",
    "1. How many trips were taken by *members* using *electric bikes*? Use `&` to combine conditions.\n",
    "2. What percentage of all trips does this represent?\n",
    "3. How many trips started at any of these three stations: \"Streeter Dr & Grand Ave\", \"DuSable Lake Shore Dr & Monroe St\", or \"Kingsbury St & Kinzie St\"? Use `isin()`.\n",
    "\n",
    "Note: Remember to use parentheses around each condition when combining with `&`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. There were 33121 members who rode electric bikes in the data set.\n",
      "2. This represents 0.33121% of trips.\n",
      "3. 2702 trips started at either 'Streeter Dr & Grand Ave', 'DuSable Lake Shore Dr & Monroe St', or 'Kingsbury St & Kinzie St'.\n"
     ]
    }
   ],
   "source": [
    "member_and_electric_mask = (rides['member_casual'] == 'member') & (rides['rideable_type'] == 'electric_bike')\n",
    "print(f\"1. There were {member_and_electric_mask.sum()} members who rode electric bikes in the data set.\")\n",
    "print(f\"2. This represents {member_and_electric_mask.sum()/len(rides)}% of trips.\")\n",
    "three_stations = rides['start_station_name'].isin(['Streeter Dr & Grand Ave', 'DuSable Lake Shore Dr & Monroe St', 'Kingsbury St & Kinzie St']) \n",
    "print(f\"3. {three_stations.sum()} trips started at either 'Streeter Dr & Grand Ave', 'DuSable Lake Shore Dr & Monroe St', or 'Kingsbury St & Kinzie St'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### Task 3b: Create Derived Columns from Datetime\n",
    "\n",
    "Add two categorical columns to the rides DataFrame based on trip start time:\n",
    "\n",
    "1. `is_weekend`: Boolean column that is True for Saturday/Sunday trips. Use .dt.dayofweek on the index (Monday=0, Sunday=6).\n",
    "2. `time_of_day`: String categories based on start hour:\n",
    "   - \"Morning Rush\" if hour is 7, 8, or 9\n",
    "   - \"Evening Rush\" if hour is 16, 17, or 18\n",
    "   - \"Midday\" for all other hours\n",
    "\n",
    "For step 2, initialize the column to \"Midday\", then use .loc[mask, 'time_of_day'] with boolean masks to assign rush hour categories. Extract hour using .dt.hour on the index.\n",
    "\n",
    "After creating both columns, use value_counts() on time_of_day to show the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_of_day\n",
      "Midday          55912\n",
      "Evening Rush    28218\n",
      "Morning Rush    15870\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "rides['is_weekend'] = (rides.index.dayofweek ==  5) | (rides.index.dayofweek == 6)\n",
    "# ChatGPT helped me with operator precedence on this one ^ and removing the .dt.\n",
    "rides['time_of_day'] = 'Midday'\n",
    "morning_rush_mask = rides.index.hour.isin([7, 8, 9])\n",
    "evening_rush_mask = rides.index.hour.isin([16, 17, 18])\n",
    "rides.loc[morning_rush_mask, 'time_of_day'] = \"Morning Rush\"\n",
    "rides.loc[evening_rush_mask, 'time_of_day'] = \"Evening Rush\"\n",
    "print(rides['time_of_day'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### Task 3c: Complex Filtering with query()\n",
    "\n",
    "Use the `query()` method to find trips that meet **all** of these criteria:\n",
    "- Casual riders (not members)\n",
    "- Weekend trips  \n",
    "- Duration greater than 20 minutes\n",
    "- Electric bikes\n",
    "\n",
    "Report:\n",
    "1. How many trips match these criteria?\n",
    "2. What percentage of all trips do they represent?\n",
    "3. What is the average duration of these trips?\n",
    "\n",
    "Hint: Column names work directly in `query()` strings. Combine conditions with `and`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. There were 1501 trips from the data set that were taken by casual riders on weekend days that lasted longer than 20 minutes and used an electric bike.\n",
      "2. Trips satisfying these conditions represent 0.01501 of all trips in the data set.\n",
      "3. The average duration of trips satisfying these conditions was 40.37352101932046 minutes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>trip_duration_min</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>time_of_day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:12:01.622</th>\n",
       "      <td>67BB74BD7667BAB7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-01 00:20:00.674</td>\n",
       "      <td>Oakley Ave &amp; Touhy Ave</td>\n",
       "      <td>bdd4c3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.012342</td>\n",
       "      <td>-87.688243</td>\n",
       "      <td>41.970000</td>\n",
       "      <td>-87.650000</td>\n",
       "      <td>casual</td>\n",
       "      <td>67.984200</td>\n",
       "      <td>False</td>\n",
       "      <td>Midday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:19:25.409</th>\n",
       "      <td>5AF1AC3BA86ED58C</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-01 00:42:09.933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Benson Ave &amp; Church St</td>\n",
       "      <td>a10cf0</td>\n",
       "      <td>42.070000</td>\n",
       "      <td>-87.730000</td>\n",
       "      <td>42.048214</td>\n",
       "      <td>-87.683485</td>\n",
       "      <td>casual</td>\n",
       "      <td>82.742067</td>\n",
       "      <td>False</td>\n",
       "      <td>Midday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:32:24.672</th>\n",
       "      <td>7961DD2FC1280CDC</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-10-01 00:23:18.647</td>\n",
       "      <td>St. Clair St &amp; Erie St</td>\n",
       "      <td>9c619a</td>\n",
       "      <td>LaSalle St &amp; Illinois St</td>\n",
       "      <td>fbd1ad</td>\n",
       "      <td>41.894345</td>\n",
       "      <td>-87.622798</td>\n",
       "      <td>41.890762</td>\n",
       "      <td>-87.631697</td>\n",
       "      <td>member</td>\n",
       "      <td>50.899583</td>\n",
       "      <td>False</td>\n",
       "      <td>Midday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:42:11.207</th>\n",
       "      <td>2E16892DEEF4CC19</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-10-01 00:10:16.831</td>\n",
       "      <td>Ashland Ave &amp; Chicago Ave</td>\n",
       "      <td>72a04d</td>\n",
       "      <td>Loomis St &amp; Archer Ave</td>\n",
       "      <td>896337</td>\n",
       "      <td>41.895954</td>\n",
       "      <td>-87.667728</td>\n",
       "      <td>41.841633</td>\n",
       "      <td>-87.657435</td>\n",
       "      <td>casual</td>\n",
       "      <td>28.093733</td>\n",
       "      <td>False</td>\n",
       "      <td>Midday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:49:25.380</th>\n",
       "      <td>AAF0220F819BEE01</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-01 00:06:27.476</td>\n",
       "      <td>900 W Harrison St</td>\n",
       "      <td>11da85</td>\n",
       "      <td>900 W Harrison St</td>\n",
       "      <td>11da85</td>\n",
       "      <td>41.874754</td>\n",
       "      <td>-87.649807</td>\n",
       "      <td>41.874754</td>\n",
       "      <td>-87.649807</td>\n",
       "      <td>member</td>\n",
       "      <td>17.034933</td>\n",
       "      <td>False</td>\n",
       "      <td>Midday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ride_id  rideable_type  \\\n",
       "started_at                                                 \n",
       "2024-09-30 23:12:01.622  67BB74BD7667BAB7  electric_bike   \n",
       "2024-09-30 23:19:25.409  5AF1AC3BA86ED58C  electric_bike   \n",
       "2024-09-30 23:32:24.672  7961DD2FC1280CDC   classic_bike   \n",
       "2024-09-30 23:42:11.207  2E16892DEEF4CC19   classic_bike   \n",
       "2024-09-30 23:49:25.380  AAF0220F819BEE01  electric_bike   \n",
       "\n",
       "                                       ended_at         start_station_name  \\\n",
       "started_at                                                                   \n",
       "2024-09-30 23:12:01.622 2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave   \n",
       "2024-09-30 23:19:25.409 2024-10-01 00:42:09.933                        NaN   \n",
       "2024-09-30 23:32:24.672 2024-10-01 00:23:18.647     St. Clair St & Erie St   \n",
       "2024-09-30 23:42:11.207 2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave   \n",
       "2024-09-30 23:49:25.380 2024-10-01 00:06:27.476          900 W Harrison St   \n",
       "\n",
       "                        start_station_id          end_station_name  \\\n",
       "started_at                                                           \n",
       "2024-09-30 23:12:01.622           bdd4c3                       NaN   \n",
       "2024-09-30 23:19:25.409              NaN    Benson Ave & Church St   \n",
       "2024-09-30 23:32:24.672           9c619a  LaSalle St & Illinois St   \n",
       "2024-09-30 23:42:11.207           72a04d    Loomis St & Archer Ave   \n",
       "2024-09-30 23:49:25.380           11da85         900 W Harrison St   \n",
       "\n",
       "                        end_station_id  start_lat  start_lng    end_lat  \\\n",
       "started_at                                                                \n",
       "2024-09-30 23:12:01.622            NaN  42.012342 -87.688243  41.970000   \n",
       "2024-09-30 23:19:25.409         a10cf0  42.070000 -87.730000  42.048214   \n",
       "2024-09-30 23:32:24.672         fbd1ad  41.894345 -87.622798  41.890762   \n",
       "2024-09-30 23:42:11.207         896337  41.895954 -87.667728  41.841633   \n",
       "2024-09-30 23:49:25.380         11da85  41.874754 -87.649807  41.874754   \n",
       "\n",
       "                           end_lng member_casual  trip_duration_min  \\\n",
       "started_at                                                            \n",
       "2024-09-30 23:12:01.622 -87.650000        casual          67.984200   \n",
       "2024-09-30 23:19:25.409 -87.683485        casual          82.742067   \n",
       "2024-09-30 23:32:24.672 -87.631697        member          50.899583   \n",
       "2024-09-30 23:42:11.207 -87.657435        casual          28.093733   \n",
       "2024-09-30 23:49:25.380 -87.649807        member          17.034933   \n",
       "\n",
       "                         is_weekend time_of_day  \n",
       "started_at                                       \n",
       "2024-09-30 23:12:01.622       False      Midday  \n",
       "2024-09-30 23:19:25.409       False      Midday  \n",
       "2024-09-30 23:32:24.672       False      Midday  \n",
       "2024-09-30 23:42:11.207       False      Midday  \n",
       "2024-09-30 23:49:25.380       False      Midday  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\"\"1. There were {len(rides.query('member_casual==\"casual\" and is_weekend==True and trip_duration_min>20 and rideable_type==\"electric_bike\"'))} trips from the data set that were taken by casual riders on weekend days that lasted longer than 20 minutes and used an electric bike.\"\"\")\n",
    "# I used ChatGPT to help me understand why triple quotes were necessary in the f string above. \n",
    "print(f\"\"\"2. Trips satisfying these conditions represent {len(rides.query('member_casual==\"casual\" and is_weekend==True and trip_duration_min>20 and rideable_type==\"electric_bike\"'))/len(rides)} of all trips in the data set.\"\"\")\n",
    "print(f\"\"\"3. The average duration of trips satisfying these conditions was {rides.query('member_casual==\"casual\" and is_weekend==True and trip_duration_min>20 and rideable_type==\"electric_bike\"')['trip_duration_min'].mean()} minutes.\"\"\")\n",
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### Task 3d: Explicit Indexing Practice\n",
    "\n",
    "Practice using `loc[]` and `iloc[]` for different selection tasks:\n",
    "\n",
    "1. Use `iloc[]` to select the first 10 trips, showing only `member_casual`, `rideable_type`, and `trip_duration_min` columns\n",
    "2. Use `loc[]` to select trips from October 15-17 (use date strings `'2024-10-15':'2024-10-17'`), showing the same three columns\n",
    "3. Count how many trips occurred during this date range\n",
    "\n",
    "Note: When using `iloc[]`, remember it's position-based (0-indexed). When using `loc[]` with the datetime index, you can slice using date strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "                          trip_duration_min                ended_at  is_weekend\n",
      "started_at                                                                    \n",
      "2024-09-30 23:12:01.622          67.984200 2024-10-01 00:20:00.674       False\n",
      "2024-09-30 23:19:25.409          82.742067 2024-10-01 00:42:09.933       False\n",
      "2024-09-30 23:32:24.672          50.899583 2024-10-01 00:23:18.647       False\n",
      "2024-09-30 23:42:11.207          28.093733 2024-10-01 00:10:16.831       False\n",
      "2024-09-30 23:49:25.380          17.034933 2024-10-01 00:06:27.476       False\n",
      "2024-09-30 23:49:40.016          13.009367 2024-10-01 00:02:40.578       False\n",
      "2024-10-01 00:00:53.414           2.598817 2024-10-01 00:03:29.343       False\n",
      "2024-10-01 00:05:44.954           0.013433 2024-10-01 00:05:45.760       False\n",
      "2024-10-01 00:06:12.035          10.472933 2024-10-01 00:16:40.411       False\n",
      "2024-10-01 00:10:03.646           7.825683 2024-10-01 00:17:53.187       False\n",
      "\n",
      "2.\n",
      "                         member_casual  rideable_type  trip_duration_min\n",
      "started_at                                                             \n",
      "2024-10-15 00:00:12.781        casual  electric_bike           2.804233\n",
      "2024-10-15 00:01:20.517        member  electric_bike          11.330867\n",
      "2024-10-15 00:05:24.811        member  electric_bike           1.868850\n",
      "2024-10-15 00:05:52.984        member  electric_bike           2.705083\n",
      "2024-10-15 00:06:18.819        member  electric_bike           1.600867\n",
      "...                               ...            ...                ...\n",
      "2024-10-17 23:45:48.739        member  electric_bike          14.703100\n",
      "2024-10-17 23:47:35.040        member  electric_bike          13.049867\n",
      "2024-10-17 23:55:34.112        member   classic_bike           3.795400\n",
      "2024-10-17 23:56:14.464        member  electric_bike          11.937217\n",
      "2024-10-17 23:59:38.103        casual   classic_bike           5.266433\n",
      "\n",
      "[7235 rows x 3 columns]\n",
      "\n",
      "3. 7235 trips occurred during this date range.\n"
     ]
    }
   ],
   "source": [
    "print(f\"1.\\n {rides.iloc[0:10, [12, 2, 13]]}\")\n",
    "print(f\"\\n2.\\n {rides.loc['2024-10-15':'2024-10-17', ['member_casual', 'rideable_type', 'trip_duration_min']]}\")\n",
    "print(f\"\\n3. {len(rides.loc['2024-10-15':'2024-10-17', ['member_casual', 'rideable_type', 'trip_duration_min']])} trips occurred during this date range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Reflect on this problem and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. `isin()` advantages: Compare using `isin(['A', 'B', 'C'])` versus `(col == 'A') | (col == 'B') | (col == 'C')`. Beyond readability, what practical advantage does `isin()` provide when filtering for many values (e.g., 20+ stations)?\n",
    "2. Conditional assignment order: In Task 3b, why did we initialize all values to \"Midday\" before assigning rush hour categories? What would go wrong if you assigned categories in a different order, or didn't set a default?\n",
    "3. `query()` vs boolean indexing: The `query()` method in Task 3c could have been written with boolean indexing instead. When would you choose `query()` over boolean indexing? When might boolean indexing be preferable despite being more verbose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "1. isin() is a much cleaner, concise alternative when checking to see if a value is present with a data set. isin()'s syntax and formatting is much easier than stringing comparisons together. Additionally, I found it quite easy to have a syntax error when stringing comparisons together due to operator precedence. When filtering for multiple values, they can all be specified in one argument rather than having to explicitily list each comparison individually.\n",
    "2. In task 3b, a majority of the hour indexes are coded as \"Midday\" (18 out of the 24 hour segments), so it makes sense to initially code everything as \"Midday\" and then further specify the categories based on hour index as needed. If you assigned categories in a different order, or didn't set a default, there is more opportunity to forget an hour index (i.e., forget to code hour 5), resulting in missing categorical data. Setting a default at least ensures all row entries will have a rush hour category, and it is quite easy to further specify hour indexes as differnt categories with boolean masks.\n",
    "3. I would choose query() over boolean indexing for shorter, simpler filter conditions. Since query can be plainly stated in clear english, it is easier to read and understand what is being filtered compared to boolean indexing which requires comparisons operands. Boolean indexing might be preferable despite being more verbose when comparing to variables (don't need to use the @ symbol as in query()) and when method chaining or other operations are required to further refine the data that needs to be filtered. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)\n",
    "\n",
    "Pandas supports a variety of indexing paradigms, including bracket notation (`df['col']`), label-based indexing (`loc[]`), and position-based indexing (`iloc[]`). The lecture recommended using bracket notation only for columns, and loc/iloc for everything else. Explain the rationale: why is this approach better than using bracket notation for everything, even though `df[0:5]` technically works for row slicing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "Label-based and position-based indexing are more explicit. Bracket notation only allows for selecting specific columns or row slicing. loc[] and iloc[] allow for precise data filtering, whereas bracket notation is limited in its functionality to pull out specific data. If a specific column of data is required, it is probably easier to recall the column name rather than the columnar positional index within the data set, explaining why bracket notation is recommended for columns only. Else, data can be more reliably, explicitly filtered by exact location parameters using iloc[] and loc[].  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Problem 4: Temporal Analysis and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Time-based patterns are crucial for understanding bike share usage. In this problem, you'll analyze when trips occur, how usage differs between rider types, and export filtered results. You'll use the datetime index you set in Problem 2 and the derived columns from Problems 2-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "#### Task 4a: Identify Temporal Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Use the datetime index to extract temporal components and identify usage patterns:\n",
    "\n",
    "1. Extract the *hour* from the index and use `value_counts()` to find the most popular hour for trips. Report the peak hour and how many trips occurred during that hour.\n",
    "2. Extract the *day name* from the index and use `value_counts()` to find the busiest day of the week. Report the day and number of trips.\n",
    "3. Sort the results from step 2 to show days in order from Monday to Sunday (not by trip count). Use `sort_index()`.\n",
    "\n",
    "Hint: Use `.dt.hour` and `.dt.day_name()` on the datetime index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The peak hour for trips occurred on the 17th hour, when 10574 trips were taken.\n",
      "2. The bussiest day of the week from the data set was Wednesday, when 16513 trips occurred.\n",
      "3. The amount of rides taken per day in Monday thru Sunday order:\n",
      "started_at\n",
      "0    11531\n",
      "1    14970\n",
      "2    16513\n",
      "3    16080\n",
      "4    13691\n",
      "5    14427\n",
      "6    12788\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#ChatGPT helped me understand how .idxmax() can be used to pull out the hour index of the most popular hour for trips.\n",
    "print(f\"1. The peak hour for trips occurred on the {rides.index.hour.value_counts().idxmax()}th hour, when {rides.index.hour.value_counts().max()} trips were taken.\")\n",
    "print(f\"2. The bussiest day of the week from the data set was {rides.index.day_name().value_counts().idxmax()}, when {rides.index.day_name().value_counts().max()} trips occurred.\")\n",
    "# Extract weekday number with .weekday accessor.\n",
    "rides.index.weekday\n",
    "# This gives the day (0 = Monday, 6 = Sunday) as a numerical value.\n",
    "print(f\"3. The amount of rides taken per day in Monday thru Sunday order:\\n{rides.index.weekday.value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Task 4b: Compare Groups with groupby()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Use `groupby()` (introduced in 07a) to compare trip characteristics across different groups:\n",
    "\n",
    "1. Calculate the average trip duration by rider type (`member_casual`). Which group takes longer trips on average?\n",
    "2. Calculate the average trip duration by bike type (`rideable_type`). Which bike type has the longest average trip?\n",
    "3. Count the number of trips by rider type using `groupby()` with `.size()`. Compare this with using `value_counts()` on the `member_casual` column - do they give the same result?\n",
    "\n",
    "Note: Use single-key groupby only (one column at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. casual riders take longer trips, averaging 23.97804585039113 minutes per trip compared to member riders who average 11.984492592450827 minutes per trip.\n",
      "2. classic_bike bikes take longer trips, averaging 20.33741016152598 minutes per trip compared to electric_bike bikes which average 12.033618178427373 minutes per trip.\n",
      "3a. Using groupby() with .size(), casual riders took 34686 trips and member riders took 65314 trips.\n",
      "3b. Using value_counts(), casual riders took 34686 trips and member riders took 65314 trips.\n",
      "3c. groupby with .size() gives the same result as using value_counts()\n"
     ]
    }
   ],
   "source": [
    "avg_trip_duration = rides.groupby('member_casual', observed=False)['trip_duration_min'].mean()\n",
    "print(f\"1. {avg_trip_duration.idxmax()} riders take longer trips, averaging {avg_trip_duration.loc[avg_trip_duration.idxmax()]} minutes per trip compared to {avg_trip_duration.idxmin()} riders who average {avg_trip_duration.loc[avg_trip_duration.idxmin()]} minutes per trip.\")\n",
    "avg_trip_duration_by_bike = rides.groupby('rideable_type', observed=False)['trip_duration_min'].mean()\n",
    "print(f\"2. {avg_trip_duration_by_bike.idxmax()} bikes take longer trips, averaging {avg_trip_duration_by_bike.loc[avg_trip_duration_by_bike.idxmax()]} minutes per trip compared to {avg_trip_duration_by_bike.idxmin()} bikes which average {avg_trip_duration_by_bike.loc[avg_trip_duration_by_bike.idxmin()]} minutes per trip.\")\n",
    "number_of_trips_by_member_type_groupby = rides.groupby('member_casual', observed=False).size()\n",
    "print(f\"3a. Using groupby() with .size(), casual riders took {number_of_trips_by_member_type_groupby.loc['casual']} trips and member riders took {number_of_trips_by_member_type_groupby.loc['member']} trips.\")\n",
    "number_of_trips_by_member_type_valuecount = rides['member_casual'].value_counts()\n",
    "print(f\"3b. Using value_counts(), casual riders took {number_of_trips_by_member_type_valuecount.loc['casual']} trips and member riders took {number_of_trips_by_member_type_valuecount.loc['member']} trips.\")\n",
    "if number_of_trips_by_member_type_groupby.loc['casual'] == number_of_trips_by_member_type_valuecount.loc['casual']:\n",
    "    print(\"3c. groupby with .size() gives the same result as using value_counts()\")\n",
    "else:\n",
    "    print(\"3c. groupby with .size() does not give the same result as using value_counts()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### Task 4c: Filter, Sample, and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Create a filtered dataset for weekend electric bike trips and export it:\n",
    "\n",
    "The provided code once again uses Path to create an `output` directory and constructs the full file path as `output/weekend_electric_trips.csv`. Use the `output_file` variable when calling `.to_csv()`.\n",
    "\n",
    "1. Filter for trips where `is_weekend == True` and `rideable_type == 'electric_bike'`\n",
    "2. Use `iloc[]` to select the first 1000 trips from this filtered dataset\n",
    "3. Use `reset_index()` to convert the datetime index back to a column (so it's included in the export)\n",
    "4. Export to CSV with filename `weekend_electric_trips.csv`, including only these columns: `started_at`, `ended_at`, `member_casual`, `trip_duration_min`, `time_of_day`\n",
    "5. Use `index=False` to avoid writing the default numeric index to the file\n",
    "\n",
    "After exporting, report how many total weekend electric bike trips existed before sampling to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 13026 total weekend electric bike trips.\n"
     ]
    }
   ],
   "source": [
    "# do not modify this setup code\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = output_dir / 'weekend_electric_trips.csv'\n",
    "\n",
    "# Task 4c code here...\n",
    "# use the variable `output_file` as the filename for step 4\n",
    "\n",
    "filtered_trips = rides.query('is_weekend==True and rideable_type==\"electric_bike\"')\n",
    "first_1000_filtered_trips = filtered_trips.iloc[0:999]\n",
    "first_1000_filtered_trips.reset_index(drop=False, inplace=True)\n",
    "first_1000_filtered_trips[['started_at', 'ended_at', 'member_casual', 'trip_duration_min', 'time_of_day']].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"There were {len(filtered_trips)} total weekend electric bike trips.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Reflect on this problem and answer the following questions:\n",
    "\n",
    "1. `groupby() conceptual model`: Explain in your own words what `groupby()` does. Use the phrase \"split-apply-combine\" in your explanation and describe what happens at each stage.\n",
    "2. `value_counts()` vs `groupby()`: In Task 4b.3, you compared two approaches for counting trips by rider type. When would you use `value_counts()` versus `groupby().size()`? Is there a situation where only one of them would work?\n",
    "3. Index management for export: In Task 4c, why did we use `reset_index()` before exporting? What would happen if you exported with the datetime index still in place and used `index=False`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "1. groupby() aggregates information based on categorical values in a specified column, allowing for increased data manipulation functionality. Using the \"split-apply-combine\" methodology, groupby() splits the data based on the categorical values of the specified column. Using the member_casual as an example, grouping by this column would split the data into two groups, the casual riders and the member riders. A function or operation can then be applied to the split data. For example, apply value_counts to the split data would tell you how many rides were taken by each type of member. Lastly, the data can be seamlessly integrated back together once the functions or operations are complete as the gropuby function doesn't inherent edit the data, just temporarily changes its \"structure\".\n",
    "2. Both value_counts() and groupby() sum the amount of each unique occurence in the specified data set. Some quick research indicates that value_counts() can only be applied to series, limiting the amount and type of data to which it can be applied. groupby() can be applied to series or DataFrames and also allows for method chaining, making it much more powerful for data analytics compared to just value_counts(). In summary. value_counts() should be used for a quick summation summary of a series where as groupby() should be used for more in depth data analysis operations on larger datasets.\n",
    "3. We used reset_index() before exporting to ensure the 'started_at' information was included as a regular column in the exported .csv file. Had we not reset the index, the 'started_at' information would not have been transferred to the .csv as expected (it would be a differently formatted column instead of \"normal\" column like the other that were exported). Had we exported with the datetime index still in place and used index=False, the 'started_at' data would have beene excluded from the .csv file entirely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "Compare `CSV` and _pickle_ formats for data storage and retrieval.\n",
    "\n",
    "Pickle is Python's built-in serialization format that saves Python objects exactly as they exist in memory, preserving all data types, structures, and metadata. Unlike CSV (which converts everything to text), pickle is binary (not human readable) and maintains the complete state of your DataFrame. Also, pickle files only work in Python, while CSV is universal. Read more in the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html).\n",
    "\n",
    "The code below investigates an interesting pattern: Do riders take longer trips from scenic lakefront stations even during rush hours? This could indicate tourists or recreational riders using these popular locations for leisure trips during typical commute times. The analysis filters for trips over 15 minutes that started from lakefront stations during morning (7-9am) or evening (4-6pm) rush hours, sorted by duration to see the longest trips first.\n",
    "\n",
    "Run the code below, then answer the interpretation questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310 long rush-hour trips from lakefront stations\n",
      "\n",
      "CSV file size: 40.57 KB\n",
      "Pickle file size: 21.18 KB\n",
      "Size difference: 19.39 KB\n",
      "\n",
      "Load time comparison:\n",
      "CSV:\n",
      "4 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Pickle:\n",
      "1.24 ms ± 149 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Data types from CSV (without parse_dates):\n",
      "started_at             object\n",
      "ended_at               object\n",
      "start_station_name     object\n",
      "end_station_name       object\n",
      "member_casual          object\n",
      "rideable_type          object\n",
      "trip_duration_min     float64\n",
      "dtype: object\n",
      "\n",
      "Data types from Pickle:\n",
      "started_at            datetime64[ns]\n",
      "ended_at              datetime64[ns]\n",
      "start_station_name            object\n",
      "end_station_name              object\n",
      "member_casual               category\n",
      "rideable_type               category\n",
      "trip_duration_min            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# the following lines were commented out since they were run in 4c\n",
    "# from pathlib import Path\n",
    "# output_dir = Path('output')\n",
    "\n",
    "csv_file = output_dir / 'lakefront_rush_trips.csv'\n",
    "pickle_file = output_dir / 'lakefront_rush_trips.pkl'\n",
    "\n",
    "# Filter for interesting pattern: Long trips (>15 min) during rush hours \n",
    "# from lakefront stations, sorted by duration\n",
    "lakefront_rush = (rides\n",
    "    .loc[(rides.index.hour.isin([7, 8, 9, 16, 17, 18]))]\n",
    "    .loc[(rides['start_station_name'].str.contains('Lake Shore|Lakefront', \n",
    "                                                    case=False, \n",
    "                                                    na=False))]\n",
    "    .loc[rides['trip_duration_min'] > 15]\n",
    "    .sort_values('trip_duration_min', ascending=False)\n",
    "    .head(1000)\n",
    "    .reset_index()\n",
    "    [['started_at', 'ended_at', 'start_station_name', 'end_station_name',\n",
    "      'member_casual', 'rideable_type', 'trip_duration_min']]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(lakefront_rush)} long rush-hour trips from lakefront stations\")\n",
    "\n",
    "# Export to both formats\n",
    "lakefront_rush.to_csv(csv_file, index=False)\n",
    "lakefront_rush.to_pickle(pickle_file)\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = os.path.getsize(csv_file) / 1024  # Convert to KB\n",
    "pickle_size = os.path.getsize(pickle_file) / 1024\n",
    "print(f\"\\nCSV file size: {csv_size:.2f} KB\")\n",
    "print(f\"Pickle file size: {pickle_size:.2f} KB\")\n",
    "print(f\"Size difference: {abs(csv_size - pickle_size):.2f} KB\")\n",
    "\n",
    "# Compare load times\n",
    "print(\"\\nLoad time comparison:\")\n",
    "print(\"CSV:\")\n",
    "%timeit pd.read_csv(csv_file)\n",
    "print(\"\\nPickle:\")\n",
    "%timeit pd.read_pickle(pickle_file)\n",
    "\n",
    "# Check data type preservation\n",
    "# Note: CSV load without parse_dates loses datetime types\n",
    "csv_loaded = pd.read_csv(csv_file)\n",
    "pickle_loaded = pd.read_pickle(pickle_file)\n",
    "\n",
    "print(\"\\nData types from CSV (without parse_dates):\")\n",
    "print(csv_loaded.dtypes)\n",
    "print(\"\\nData types from Pickle:\")\n",
    "print(pickle_loaded.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "After running the code, answer these questions:\n",
    "\n",
    "1. Method chaining: The analysis uses method chaining with a specific formatting pattern:\n",
    "\n",
    "   ```python\n",
    "   result = (df\n",
    "       .method1()\n",
    "       .method2()\n",
    "       .method3()\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   This wraps the entire chain in parentheses, allowing each method to appear on its own line without backslashes. Discuss why this makes formatting more readable, how it makes debugging easier, how it relates to seeing changes in the code with git diff, and what downsides heavy chaining might have.\n",
    "3. Data types: Compare the dtypes from CSV versus pickle. What types were preserved by pickle that were lost in CSV? Why is this preservation significant for subsequent analysis?\n",
    "4. Trade-offs: Given your observations about size, speed, and type preservation, when would you choose pickle over CSV for your work? When would CSV still be the better choice despite pickle's advantages?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "*Graduate follow-up interpretation here*\n",
    "1. This makes formatting more readable as it is easier to identify where each individual method operations starts and stops, compared to one long string which would require a more stringent review to identify each method and the order in which they are applied. Debugging capabilites improve since each method is on its own line, if python indicates which line the error occured on, the programmer can easily identify which method has the error. If there was a single long line of method chaining, it could take more evaluation and review time to determine and identify exactly which method is hte culprit. Seeing changes with git and diff would be more easily digestible since the changes would be displayed vertically, line by line, rather than identifying the changes with various colors horizontally in one long line of code. The downside of heavy method chaining is that there is no intermediary if data or a value prior to the final result needs o be extracted. A new variable or line of code needs to be inserted truncating the method chaining where the new value is derived. Heavy method chaining could also increase the probability of an error in code, as I know I am still learning what operations can be used as methods or attributes, so if I could easily see myself trying to use a function that is not compatible with method chaining.\n",
    "2. It looks like pickle preserved the modified data type of each column whereas the .CSV file converted everything to an object data type. More specifically, the datetime64[ns] and category data types were preserved by pickle, while .CSV only supported object and float64 data types. This preservation is significant for subsequent analysis as it allows for more powerful data manipulation (with the specialized data types) and also improves future memory usage and code efficiency as the specialized data types require less memory storage.\n",
    "3. It seems like pickle should be used on larger dataset where work will solely be performed in python. .CSV should be used on smaller data sets where memory usage and code efficiency are not of paramount importance. Additionally, .CSV should be used if data is to be shared across multiple platforms and collaborators, as it allows for more flexibility in which tools are used for data analysis. Finally, .CSV's human readibility allows for visual verification, where as python's pickle is binary and not human readible, which may make smore users uncomfortable not being able to visually see the data preservation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Address the following questions in a markdown cell:\n",
    "\n",
    "1. NumPy vs Pandas\n",
    "   - What was the biggest conceptual shift moving from NumPy arrays to Pandas DataFrames?\n",
    "   - Which Pandas concept was most challenging: indexing (loc/iloc), missing data, datetime operations, or method chaining? How did you work through it?\n",
    "2. Real Data Experience\n",
    "   - How did working with real CSV data (with missing values, datetime strings, etc.) differ from hw2b's synthetic NumPy arrays?\n",
    "   - Based on this assignment, what makes Pandas well-suited for data analysis compared to pure NumPy?\n",
    "3. Learning & Application\n",
    "   - Which new skill from this assignment will be most useful for your own data work?\n",
    "   - On a scale of 1-10, how prepared do you feel to use Pandas for your own projects? What would increase that score?\n",
    "4. Feedback\n",
    "   - Time spent: ___ hours (breakdown optional)\n",
    "   - Most helpful part of the assignment: ___\n",
    "   - One specific improvement suggestion: ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "The biggest conceptual shift moving from NumPy arrays to Pandas DataFrames was the heterogeneity of Pandas DataFrames compared to the homogeneity of NumPy arrays. Additionally, having non-numeric values was also another significant conceptual shift, as most use-cases with NumPy arrays were primarily concerned with numerical operations only, whereas Pandas DataFrames allows for categorical and datetime data analysis.\n",
    "\n",
    "What has been the most challenging for me has been the .dt accessor operations. I really struggled trying to understand how these operations were applied to the index in previous problems. I understood how to filter the value_counts by chronological day, however, only by converting the day to a numerical index. I could not figure out how to convert the numerical index back to the name of the day. Another concept I struggled with was the object output of data after it was filtered or aggregated. I still don't fully understand what the output of value_counts() or groupby().size() actually is and how that can be indexed to get individual values.\n",
    "\n",
    "2.\n",
    "\n",
    "Working with real CSV data differed from working with synthetic NumPy arrays in HW2b as it better reflected what kind of data you would expect to work with in industry. The synthetic NumPy arrays more closely aligned with idealized data, where real life data like the .CSV file we worked had missing data and could have other discrepancies. Working with real life data also allowed for more organic data analysis operations that are harder to develop with conceptualized data.\n",
    "\n",
    "Based on this assignment, the primary difference between Pandas and NumPy that makes Pandas more well-suited for data analysis as compared to NumPy is Pandas ability to accept heterogeneous, non-numerical data types (such as mixtures of strings, datetime objects, and categorical objects) into it's DataFrames, allowing for analysis to be performed on categories, dates, stations, etc., compared to just numerical analysis (station ID, longitude and latitude data).\n",
    "\n",
    "3.\n",
    "\n",
    "Importing a CSV, analyzing the data to provide more insightful information, and exporting the CSV file in a user-designed format for application specific uses is a new skill from this assignment that will be most useful for my own data work. I can already vision how I can use these skills for analyzing supplier performance data and outputting it for leadership presentations.\n",
    "\n",
    "On a scale of 1-10, I feel about a 7 to start using Pandas in my own projects. This assignment was my first introductin to Pandas, so more frequent use of this language would make me feel more comfortable. Maybe instead of one large assignment, if it was broken up into a few smaller assignments so we used the language in shorter burts but more frequently over a longer time period I would feel more comfortable. I really only worked on this assignment this past weekend, so all-in-all I've had two days of Pandas experience. \n",
    "\n",
    "4.\n",
    "\n",
    "Time spent: ~10-12 hours.\n",
    "Most helpful part of the assignment: Providing hints and directly specifying which pandas methods or operations to use.\n",
    "One specific improvement suggestion: Release this homework assignment as we are learning the material and provide expected outputs so students know how to format final results / if they are performing the analysis correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
